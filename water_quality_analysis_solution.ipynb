{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**PROJECT 1 : WATER QUALITY ANALYSIS SOLUTION**\n",
    "\n",
    "20th July 2024\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description and Task Overview\n",
    "\n",
    "## Data Description\n",
    "\n",
    "This dataset contains measurements of various water quality parameters collected from different water sources over time. Below is a detailed description of each column in the dataset:\n",
    "\n",
    "1. **Index**: Unique identifier for each data entry.\n",
    "2. **pH**: The pH level of the water, indicating its acidity or alkalinity.\n",
    "3. **Iron**: Concentration of iron in the water (in mg/L).\n",
    "4. **Nitrate**: Concentration of nitrate in the water (in mg/L).\n",
    "5. **Chloride**: Concentration of chloride in the water (in mg/L).\n",
    "6. **Lead**: Concentration of lead in the water (in mg/L).\n",
    "7. **Zinc**: Concentration of zinc in the water (in mg/L).\n",
    "8. **Color**: The color of the water sample (e.g., Colorless, Faint Yellow).\n",
    "9. **Turbidity**: The cloudiness or haziness of the water, measured in Nephelometric Turbidity Units (NTU).\n",
    "10. **Fluoride**: Concentration of fluoride in the water (in mg/L).\n",
    "11. **Copper**: Concentration of copper in the water (in mg/L).\n",
    "12. **Odor**: Descriptive term for the odor of the water (e.g., odorless, faint odor).\n",
    "13. **Sulfate**: Concentration of sulfate in the water (in mg/L).\n",
    "14. **Conductivity**: The ability of the water to conduct electricity, measured in microsiemens per centimeter (µS/cm).\n",
    "15. **Chlorine**: Concentration of chlorine in the water (in mg/L).\n",
    "16. **Manganese**: Concentration of manganese in the water (in mg/L).\n",
    "17. **Total Dissolved Solids (TDS)**: Total concentration of dissolved substances in the water (in mg/L).\n",
    "18. **Source**: Source of the water sample (e.g., Lake, River, Ground).\n",
    "19. **Water Temperature**: Temperature of the water at the time of measurement (in °C).\n",
    "20. **Air Temperature**: Temperature of the air at the time of measurement (in °C).\n",
    "21. **Month**: Month when the sample was taken.\n",
    "22. **Day**: Day of the month when the sample was taken.\n",
    "23. **Time of Day**: Time of day when the sample was taken (e.g., 0 for midnight, 12 for noon).\n",
    "24. **Target**: Binary target variable indicating whether the water quality meets a certain standard (0 for meets standard, 1 for does not meet standard).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task for Participants\n",
    "\n",
    "Participants are tasked with analyzing the water quality data to determine patterns, correlations, and insights that can help in understanding and managing water quality. Specific tasks include:\n",
    "\n",
    "1. **Data Cleaning and Preprocessing**:\n",
    "    - Handle missing or inconsistent data.\n",
    "    - Convert categorical data into numerical format if necessary.\n",
    "    - Normalize or standardize numerical features.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**:\n",
    "    - Generate summary statistics for each feature.\n",
    "    - Visualize data distributions and relationships between variables using plots (e.g., histograms, scatter plots, box plots).\n",
    "    - Identify any trends or anomalies in the data.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "    - Create new features that might be useful for prediction, such as interaction terms or aggregates.\n",
    "    - Evaluate the importance of different features for predicting water quality.\n",
    "\n",
    "4. **Model Building**:\n",
    "    - Split the data into training and testing sets.\n",
    "    - Train machine learning models to predict the target variable (water quality standard).\n",
    "    - Evaluate model performance using appropriate metrics (e.g., accuracy, precision, recall, F1-score).\n",
    "\n",
    "5. **Model Interpretation and Insights**:\n",
    "    - Interpret the model to understand which factors most influence water quality.\n",
    "    - Provide actionable insights based on model findings.\n",
    "    - Discuss potential interventions or policy recommendations to improve water quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Access**\n",
    "\n",
    "Download the subset `water_quality_dataset_100k_new.csv` dataset from the google drive\n",
    "\n",
    "https://drive.google.com/file/d/1NPnyvMMa4EgRMpCWd1LOOVGeOOmtcC6f/view?usp=drive_link\n",
    "\n",
    "\n",
    "`Original dataset` https://www.kaggle.com/datasets/mitanshuchakrawarty/water-quality-prediction/data?select=dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Neccessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,  ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read Data**\n",
    "![](image/data_snapshot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "data= pd.read_csv('water_quality_dataset_100k_new.csv')\n",
    "data = data.drop(columns= \"Index\")\n",
    "df=data.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_count=df['Target'].value_counts()\n",
    "plt.pie(target_count, labels=target_count.index, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove(\"Target\")\n",
    "numerical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "# Plot histograms for numerical features\n",
    "df[numerical_features].hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initial visualization of distributions and relationships\n",
    "print(\"\\nPairplot of Numerical Features and Target:\")\n",
    "# sns.pairplot(df[numerical_features + ['Target']], diag_kind='kde', hue='Target')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBoxplots of Numerical Features:\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=df[numerical_features], orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for numerical features\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, column in enumerate(df[numerical_features], 1):\n",
    "    plt.subplot(6, 6, i)\n",
    "    sns.boxplot(data=df[numerical_features], x=column)\n",
    "    plt.title(column)\n",
    "\n",
    "plt.suptitle('Box Plots of Numerical Features', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCount plots of Categorical Features:\")\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.countplot(data=df, x=feature, hue='Target')\n",
    "    plt.title(f'Count Plot of {feature} with Target')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter plots for pairs of numerical features\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Example pairs of numerical features\n",
    "pairs = [('pH', 'Iron'), ('Nitrate', 'Chloride'), ('Lead', 'Zinc'), ('Fluoride', 'Copper'), \n",
    "         ('Total Dissolved Solids', 'Water Temperature'), ('Water Temperature', 'Air Temperature'),\n",
    "         ]\n",
    "\n",
    "for i, (x_feature, y_feature) in enumerate(pairs, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.scatterplot(x=x_feature, y=y_feature, data=df, alpha=0.5, hue='Target')\n",
    "\n",
    "    plt.xlabel(x_feature)\n",
    "    plt.ylabel(y_feature)\n",
    "    plt.title(f'{x_feature} vs {y_feature}')\n",
    "    \n",
    "\n",
    "plt.suptitle('Scatter Plots of Selected Feature Pairs', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCorrelation Matrix of Numerical Features:\")\n",
    "plt.figure(figsize=(20, 8))\n",
    "corr_matrix = df[numerical_features + [\"Target\"]].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1: Data Cleaning and Preprocessing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Handle Missing or Inconsistent Data\n",
    "\n",
    "1. **Identify Missing Values:**\n",
    "   - Detect missing values in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing value per rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing=df.isna().sum(axis=1)/df.shape[1] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Display missing values count\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "print(\"Missing Values Count:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Impute or Remove Missing Values:**\n",
    "   - For numerical features, you can impute missing values using mean, median, or mode.\n",
    "   - For categorical features, you can impute missing values using the most frequent category or a placeholder such as 'Unknown'.\n",
    "   - Alternatively, remove rows with missing values if they are few and do not significantly impact the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the percentage of missing values, we can decide on the following strategies:\n",
    "\n",
    "- if a column has a very high percentage of missing values, it might be best to drop it.\n",
    "- For numerical columns with moderate missing values, we can fill them with the mean or median.\n",
    "- For categorical columns with missing values, we can fill them with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for step 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for numerical features with the mean\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numerical_features] = df[numerical_features].fillna(df[numerical_features].mean())\n",
    "\n",
    "# Verify if missing values are imputed\n",
    "print(df[numerical_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_features] = df[categorical_features].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for categorical features with the most frequent category\n",
    "# categorical_features = df.select_dtypes(include=['object']).columns\n",
    "# df[categorical_features] = df[categorical_features].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "# Verify if missing values are imputed\n",
    "print(df[categorical_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Display missing values count\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "print(\"Missing Values Count:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert Categorical Data into Numerical Format\n",
    "\n",
    "1. **Identify Categorical Features:**\n",
    "   - Features like `Color`, `Odor`, and `Source` are categorical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Encode Categorical Features:**\n",
    "   - Use one-hot encoding for nominal categorical features (e.g., `Color`, `Odor`, `Source`).\n",
    "   - Use label encoding for ordinal categorical features, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical features dynamically\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Exclude the target and non-feature columns if present\n",
    "non_feature_cols = ['Target', 'Month', 'Day', 'Time of Day']\n",
    "numerical_features = [col for col in numerical_features if col not in non_feature_cols]\n",
    "categorical_features = [col for col in categorical_features if col not in non_feature_cols]\n",
    "\n",
    "# Visualize initial dataframe\n",
    "print(\"Initial DataFrame:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_features].hist(figsize=(15, 12), bins=15)\n",
    "plt.suptitle('Histograms of Numerical Features Before Transformation', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Normalize or Standardize Numerical Features\n",
    "\n",
    "1. **Identify Numerical Features:**\n",
    "   - Features like `pH`, `Iron`, `Nitrate`, `Chloride`, etc., are numerical.\n",
    "\n",
    "2. **Normalize or Standardize:**\n",
    "   - Normalize the features using Min-Max scaling or Standardize the features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize or Standardize Numerical Features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', MinMaxScaler())  # Normalize using Min-Max scaling\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert Categorical Data into Numerical Format\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encode categorical features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformations to the dataframe\n",
    "df_preprocessed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Debug: Check the shape of the transformed data\n",
    "print(f\"Shape of transformed data: {df_preprocessed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed data:\")\n",
    "print(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "num_features = numerical_features\n",
    "cat_features = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "all_features = num_features + list(cat_features)\n",
    "\n",
    "# Debug: Check the feature names\n",
    "print(f\"Number of feature names: {len(all_features)}\")\n",
    "print(f\"Feature names: {all_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_preprocessed = pd.DataFrame(df_preprocessed, columns=all_features)\n",
    "\n",
    "# Add back non-transformed columns (if any)\n",
    "df_preprocessed[non_feature_cols] = df[non_feature_cols].reset_index(drop=True)\n",
    "\n",
    "df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the preprocessed dataframe\n",
    "print(\"\\nPreprocessed DataFrame:\")\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "1. SimpleImputer is used to handle missing values.\n",
    "2. OneHotEncoder is used for encoding categorical variables.\n",
    "3. MinMaxScaler is used to normalize numerical features.\n",
    "4. ColumnTransformer is used to apply different preprocessing steps to different columns.\n",
    "\n",
    "Ensure to replace 'your_data.csv' with the actual file path of your dataset. The preprocessed data will be saved in a new file called cleaned_data.csv. This cleaned and preprocessed data is now ready for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Task 2. Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Step 1: Generate Summary Statistics for Each Feature\n",
    "\n",
    "1. **Summary Statistics:**\n",
    "   - Use descriptive statistics to summarize the central tendency, dispersion, and shape of the dataset’s distribution for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize Data Distributions and Relationships Between Variables\n",
    "\n",
    "1. **Histograms:**\n",
    "   - Visualize the distribution of numerical features.\n",
    "\n",
    "2. **Box Plots:**\n",
    "   - Identify outliers and understand the distribution of numerical features.\n",
    "\n",
    "3. **Scatter Plots:**\n",
    "   - Examine relationships between pairs of numerical features.\n",
    "\n",
    "4. **Line Plots:**\n",
    "   - Examine trends in numerical features.\n",
    "\n",
    "5. **Correlation Matrix and Heatmap:**\n",
    "   - Show the correlation between numerical features.\n",
    "\n",
    "6. **Count Plots:**\n",
    "   - Visualize the frequency of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns for correlation matrix\n",
    "numerical_columns_for_corr = df_preprocessed.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Visualize distributions of numerical features after transformation\n",
    "df_preprocessed[numerical_columns_for_corr].hist(figsize=(20, 15), bins=15,)\n",
    "plt.suptitle('Histograms of Numerical Features After Transformation', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBoxplots of Numerical Features:\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=df_preprocessed.drop(['Time of Day', 'Day'], axis=1), orient='h')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 12))\n",
    "df_preprocessed.drop(['Time of Day', 'Day'], axis=1).boxplot()\n",
    "plt.title('Box Plots of Numerical Features', fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations in the preprocessed data\n",
    "plt.figure(figsize=(14, 12))\n",
    "correlation_matrix = df_preprocessed[numerical_columns_for_corr].corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Preprocessed Data', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "significant_correlations = correlation_matrix[(correlation_matrix > threshold) & (correlation_matrix != 1.0)].dropna(how='all').dropna(axis=1, how='all')\n",
    "\n",
    "# Show significant correlations\n",
    "print(\"\\nSignificant Correlations:\")\n",
    "significant_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize significant correlations in the numerical data including the target column\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(significant_correlations, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Significant Correlations in Numerical Data Including Target', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Identify Trends or Anomalies in the Data\n",
    "\n",
    "1. **Trend Analysis:**\n",
    "   - Look for patterns or trends over time or other categorical dimensions.\n",
    "\n",
    "2. **Anomaly Detection:**\n",
    "   - Identify any anomalies or unusual observations in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "1. Descriptive Statistics: Use describe() method to generate summary statistics for each feature.\n",
    "2. Histograms: Use hist() method to visualize the distribution of numerical features.\n",
    "3. Box Plots: Use boxplot() function to visualize outliers and understand the distribution of numerical features.\n",
    "4. Scatter Plots: Use pairplot() function from Seaborn to examine relationships between pairs of numerical features.\n",
    "5. Correlation Matrix and Heatmap: Use corr() method and heatmap() function from Seaborn to visualize the correlation between numerical features.\n",
    "6. Count Plots: Use countplot() function from Seaborn to visualize the frequency of categorical features.\n",
    "7. Trend Analysis: Group data by month and plot average water temperature over time to identify trends.\n",
    "8. Anomaly Detection: Use box plots to identify outliers in numerical features.\n",
    "9. Ensure to replace 'your_data.csv' with the actual file path of your dataset. These visualizations and analyses will help you understand the distributions, relationships, and potential anomalies in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.to_csv(\"preprocessed.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Task 3. Feature Engineering** (OPTIONAL - Increase the quality of the dataset)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create New Features\n",
    "\n",
    "1. **Interaction Terms:**\n",
    "   - Create new features by multiplying or combining existing features to capture interactions between them.\n",
    "\n",
    "2. **Aggregate Features:**\n",
    "   - Create aggregate features such as mean, median, sum, or count of other features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove(\"Target\")\n",
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "\n",
    "# Step 3: Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# Create new features\n",
    "df['Water_Temp_to_Air_Temp_Ratio'] = df['Water Temperature'] / (df['Air Temperature'] + 1)\n",
    "df['Total_Metals'] = df[['Iron', 'Lead', 'Zinc', 'Copper', 'Manganese']].sum(axis=1)\n",
    "\n",
    "numerical_features.extend(['Water_Temp_to_Air_Temp_Ratio', 'Total_Metals'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_features.remove('Day')\n",
    "# numerical_features.remove('Time of Day')\n",
    "# print(numerical_features)\n",
    "# categorical_features.remove('Month')\n",
    "# categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply transformations\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame from the transformed data\n",
    "# X_transformed_df = pd.DataFrame(X, columns=numerical_features + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)))\n",
    "\n",
    "\n",
    "X_transformed_df=X[numerical_features].copy()\n",
    "print(\"\\nTransformed Features Head:\")\n",
    "X_transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Task 4. Model Building**\n",
    "## Step 1: Split the Data into Training and Testing Sets\n",
    "   - Use a function to split the dataset into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed_df, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Machine Learning Models to Predict the Target Variable\n",
    "\n",
    "1. **Train Models:**\n",
    "   - Train different machine learning models such as Logistic Regression, Random Forest, and Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "model_rf=RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate Model Performance Using Appropriate Metrics\n",
    "\n",
    "1. **Evaluate Models:**\n",
    "   - Evaluate the performance of the models using metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "\n",
    "y_pred = model_rf.predict(X_test)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "results= {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to get results to avoid repetition\n",
    "def get_results(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    results= {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model with progress tracking using tqdm\n",
    "results = {}\n",
    "for name, model in tqdm(models.items(), desc=\"Training and Evaluating Models\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results[name] = get_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot results for visual comparison\n",
    "results_df.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the best model based on F1-score\n",
    "best_model_name = results_df['f1'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate and plot the confusion matrix\n",
    "def plot_confusion_matrix (model, X_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix for {model_name} Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix (model_rf, X_test, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 5. Model Interpretation and Insights**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Interpret the Model to Understand Which Factors Most Influence Water Quality\n",
    "1. **Feature Importance:**\n",
    "   - Use the feature importance scores from tree-based models like Random Forest to identify which factors most influence water quality.\n",
    "   - Use coefficients from Logistic Regression to understand the direction and magnitude of influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Model Interpretation and Insights\n",
    "# ==============================\n",
    "\n",
    "# Interpret the best model to understand which factors most influence water quality\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # feature_importance = pd.Series(best_model.feature_importances_, index=numerical_features + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)))\n",
    "    feature_importance = pd.Series(best_model.feature_importances_, index=numerical_features )\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title(f'Feature Importance in Best Model - {best_model}')\n",
    "    plt.show()\n",
    "\n",
    "important_features = feature_importance.head(10).index if 'feature_importances_' in dir(best_model) else numerical_features + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))\n",
    "print(f'\\nTop 5 important features: {important_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter plots for pairs of numerical features\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Example pairs of numerical features\n",
    "pairs = [('pH', 'Turbidity'), ('Nitrate', 'Chloride'), ('Manganese', 'Copper'), ('Nitrate', 'Copper'), \n",
    "\n",
    "         ('Manganese', 'pH'), ('Turbidity', 'Chloride')\n",
    "         ]\n",
    "\n",
    "for i, (x_feature, y_feature) in enumerate(pairs, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.scatterplot(x=x_feature, y=y_feature, data=df, alpha=0.5, hue='Target')\n",
    "    plt.xlabel(x_feature)\n",
    "    plt.ylabel(y_feature)\n",
    "    plt.title(f'{x_feature} vs {y_feature}')\n",
    "    \n",
    "\n",
    "plt.suptitle('Scatter Plots of Selected Feature Pairs', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Provide Actionable Insights Based on Model Findings\n",
    "\n",
    "1. **Identify Key Factors:**\n",
    "   - Determine the most important factors influencing water quality based on the model interpretation.\n",
    "\n",
    "2. **Actionable Insights:**\n",
    "   - Provide specific recommendations for addressing key factors to improve water quality.\n",
    "\n",
    "## Step 3: Discuss Potential Interventions or Policy Recommendations to Improve Water Quality\n",
    "\n",
    "1. **Interventions:**\n",
    "   - Suggest practical measures that can be taken to address the key factors influencing water quality.\n",
    "\n",
    "2. **Policy Recommendations:**\n",
    "   - Propose policy changes or initiatives to support the interventions and improve water quality on a broader scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide actionable insights based on model findings\n",
    "insights = f\"\"\"\n",
    "Based on the best model, the most influential factors for water quality are:\n",
    "{', '.join(important_features.tolist())}\n",
    "\n",
    "Policy recommendations:\n",
    "- Regular monitoring and regulation of `{', '.join(important_features.tolist())}` levels in water sources.\n",
    "- Implementing measures to reduce industrial and agricultural runoff to control Nitrate and Sulfate levels.\n",
    "- Public awareness campaigns on the importance of water quality and ways to reduce contamination.\n",
    "\"\"\"\n",
    "print(insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their hyperparameters for grid search\n",
    "model_params = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'solver': ['liblinear', 'lbfgs']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [10, 50, 100],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and evaluate each model with hyperparameter tuning\n",
    "results = {}\n",
    "for name, mp in tqdm(model_params.items(), desc=\"Training and Evaluating Models\"):\n",
    "    grid_search = GridSearchCV(mp['model'], mp['params'], cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    results[name] = get_results (y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    best_params = metrics.get('best_params', 'N/A')\n",
    "    print(f\"  Best Parameters: {best_params}\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        if metric_name != 'best_params':\n",
    "            print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb = XGBClassifier(random_state=42, eval_metric='logloss') \n",
    "xgb.fit(X_train.values, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "\n",
    "y_pred = xgb.predict(X_test.values)\n",
    "    \n",
    "get_results (y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "# Perform Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train.values, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_xgb.predict(X_test.values)\n",
    "              \n",
    "\n",
    "# Print the best parameters and the evaluation metrics\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "results= {\n",
    "    \"Best Parameters\": grid_search.best_params_}\n",
    "get_results (y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=42, \n",
    "                    colsample_bytree= 1.0,\n",
    "                    learning_rate= 0.1,\n",
    "                    max_depth= 7,\n",
    "                    n_estimators= 50,\n",
    "                    subsample= 0.8, eval_metric='logloss') \n",
    "xgb.fit(X_train.values, y_train)\n",
    "y_pred = xgb.predict(X_test.values)\n",
    "get_results (y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix (model_rf, X_test, 'Random forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate and plot the confusion matrix\n",
    "plot_confusion_matrix (xgb, X_test.values, 'XGBOOST')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
